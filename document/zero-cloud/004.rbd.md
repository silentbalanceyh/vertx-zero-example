---
title: 执子之手：Ceph RBD
---

# 执子之手：Ceph RBD

> 死生契阔，与子成说。执子之手，与子偕老。——《诗经》

&ensp;&ensp;&ensp;&ensp;前边三个章节我们已经搭建好了K8S、Ceph、Harbor，基础环境已经有了，接下来这个章节就让K8S和Ceph联动起来，将Ceph作为K8S的底层存储引擎让二者协同工作，不论是之后的KubeSphere还是TiDB都可以直接使用本章配置的结果来访问底层存储。

![](./_image/2022-11-30/20221130092721.png)

&ensp;&ensp;&ensp;&ensp;K8S和Ceph集成有三种实现方式：

* Volumes存储卷
* PV/PVC持久化卷/持久化卷声明
* StorageClass动态存储，动态创建PV、PVC

&ensp;&ensp;&ensp;&ensp;Ceph支持K8S存储有两种类型：

* CephFS：文件系统
* Ceph RBD：块存储

> **注意**：一般写CephFS会将Ceph和FS连写，而RBD则中间加一空白来写！

## 1. ceph-csi

&ensp;&ensp;&ensp;&ensp;CSI[^1]全称Container Storage Interface，它是Ceph的一个插件，它的目的是定义行业标准“容器存储接口”，使存储供应商（SP）可以开发一个符合CSI标准的插件并使其能够在多个容器编排（CO）系统中工作。

&ensp;&ensp;&ensp;&ensp;本章K8S通过`ceph-csi`（csi plugin）来接入ceph存储（csi相关组件的分析以rbd为例进行分析），对csi系统结构、涉及的k8s对象和组件进行简单介绍，以及k8s对存储进行相关操作流程分析，存储相关操作包括**存储创建、存储扩容、存储挂载、解除存储、存储删除**操作。

### 1.1. 整体架构

> 图参考文档中内容重绘，PV/PVC/SC此处不解释其作用（烂大街的概念），下文主要围绕CSI部分。

![](./_image/2022-11-30/20221130113616.png)

#### VolumeAttachment组件

&ensp;&ensp;&ensp;&ensp;该组件记录了PV挂载的相关信息：**挂载到哪个Node节点、由哪个Volume Plugin来挂载**等。AD Controller会创建一个VolumeAttachment，而External-Attacher通过观察该VolumeAttachment，根据状态属性来执行存储的挂载、卸载操作。

#### CSINode

&ensp;&ensp;&ensp;&ensp;**CSINode**记录了`csi plugin`相关信息（**NodeId、DriverName、拓扑**），当Node Driver Register向kubelet注册一个csi plugin之后，会创建（更新）一个CSINode对象，记录csi plugin的相关信息。

#### Volume Plugin

&ensp;&ensp;&ensp;&ensp;该组件扩展各种存储类型的卷管理能力，实现第三方存储各种操作能力和K8S存储系统结合，并调用第三方存储的接口或命令，从而提供数据卷的创建/删除、attach/detach、mount/umount具体操作实现，可以认为是第三方存储代理人。

* **in-tree**：在K8S源码内部实现，和K8S一起发布、管理，更新迭代慢、灵活性差。
* **out-of-tree**：代码独立于K8S，由存储厂商实现，有`csi, flexvolume`两种实现。

#### CSI Plugin

&ensp;&ensp;&ensp;&ensp;`ceph-csi`就属于csi plugin，它主要分为ControllerServer和NodeServer，负责不同的存储操作。

#### External Plugin

&ensp;&ensp;&ensp;&ensp;主要包含四个子组件（参考图），它辅助csi plugin组件共同完成存储相关的操作。

|组件|作用|
|---|---|
|provisioner|Watch PVC对象，调用csi plugin创建存储，最终创建PV。|
|attacher|Watch VolumeAttachment对象，调用csi plugin做attach/dettach操作。|
|resizer|Watch PVC对象，调用csi plugin来做存储扩容。|
|snapshotter|创建快照。|

#### Node-Driver-Registrar

&ensp;&ensp;&ensp;&ensp;该组件负责实现csi plugin（NodeServer）的注册，让kubelet感知csi plugin的存在。

#### AD/PV Controller

&ensp;&ensp;&ensp;&ensp;PV Controller负责PV、PVC绑定和生命周期管理（创建/删除底层存储、创建/删除PV对象、PV和PVC对象状态变更）。

&ensp;&ensp;&ensp;&ensp;AD Controller全称Attachment/Detachment控制器，主要负责创建、删除VolumeAttachment对象，并调用volume plugin来做存储设备的Attach/Detach操作（将数据卷挂载到特定node节点上/从特定node节点上解除挂载），以及更新node.Status.VolumesAttached等。

> 关于这部分内容参考引用博文第一章，这里就不再做过多介绍，本章节的基本介绍只是起一个抛砖引玉的作用。

### 1.2. CSI插件

&ensp;&ensp;&ensp;&ensp;ceph-csi的块设备功能要`K8S v1.13`以上或更高版本才可用，它动态提供了RBD映像以支持K8S卷并将这些RBD映像映射为工作节点上的块设备（可选地挂载映像中包含的文件系统）运行引用RBD支持卷的Pod。它的整体结构如下：

![](./_image/2022-11-30/20221130122943.png)

&ensp;&ensp;&ensp;&ensp;所以您可以通过kubernetes sidecar部署provisioner, attacher, resizer, driver-registrar, snapshotter组件以支持CSI功能。ceph-csi插件实现了支持CSI的Container Orchestrator（OC）和Ceph集群之间的接口，允许动态供应Ceph卷并将它们附加到工作负载中。默认情况下ceph-csi使用RBD内核模块，可能不支持所有的Ceph CRUSH可调参数或RBD映像等特性。

<hr/>

## 2. 部署步骤

### 2.1.Ceph: Pool

1. 默认情况下，Ceph块设备使用rbd池，使用下边命令为K8S创建一个池：

    ```shell
    # 存储池名称：k8s.store
    # 在目前三节点的集群中，推荐后边不使用128 128，而是32 32两个值，否则等待更新的时间会很长
    ceph osd pool create k8s.store 128 128
    # 新创建的池必须在使用之前初始化，此时需要使用rbd工具初始化
    rbd pool init k8s.store
    ```

    ![](./_image/2022-11-30/20221130133600.png)

    您可以登录Ceph集群管理界面等待任务执行完成（截图中可以看到**Applications类型**是`rbd`）：

    ![](./_image/2022-11-30/20221130133811.png)

2. 为K8S创建新的Ceph账号

    ```shell
    # 账号信息
    ceph auth get-or-create client.k8s \
        mon 'profile rbd' \
        osd 'profile rbd pool=k8s.store' \
        mgr 'profile rbd pool=k8s.store'
    # 查看创建账号
    ceph auth list
    ```

    ![](./_image/2022-11-30/20221130140504.png)

    ![](./_image/2022-11-30/20221130140558.png)

3. 账号创建完成后，ceph-csi需要一个存储在K8S中的ConfigMap对象来定义Ceph集群的Ceph mon地址

    ```shell
    ceph mon dump
    ceph -s|grep mon
    ```

    ![](./_image/2022-11-30/20221130140857.png)

### 2.2.K8S: ConfigMap

1. 将部署配置放在`~/deploy/ceph`中，clusterID需要和上边取得的id保持一致，并且需要修改mon地址，多个地址后面要使用逗号分隔。

    ```shell
    cd /root/deploy/ceph
    vim config-map-csi.yaml
    ```

    文件内容如下：

    ```yaml
    # 文件：config-map-csi.yaml
    # 名称：ceph-csi-config
    apiVersion: v1
    kind: ConfigMap
    data:
        config.json: |-
            [
                {
                    "clusterID":"79ed3cf4-6a04-11ed-8db2-fa163ec820ba",
                    "monitors":[
                        "192.168.0.154:6789",
                        "192.168.0.123:6789",
                        "192.168.0.208:6789"
                    ]
                }
            ]
    metadata:
        name: ceph-csi-config
    ```

    ![](./_image/2022-11-30/20221130141828.png)

2. 在K8S中创建该ConfigMap对象：

    ```shell
    kubectl apply -f config-map-csi.yaml
    kubectl get cm ceph-csi-config
    ```

    ![](./_image/2022-11-30/20221130141915.png)

3. 按照步骤4-5创建额外的几个ConfigMap（必须）

    ```yaml
    # 文件：config-map-csi-kms.yaml
    # 名称：ceph-csi-encryption-kms-config
    apiVersion: v1
    kind: ConfigMap
    data:
        config.json: |-
            {}
    metadata:
        name: ceph-csi-encryption-kms-config
    
    ------
    # 文件：config-map-ceph.yaml
    # 名称：ceph-config
    apiVersion: v1
    kind: ConfigMap
    data:
        ceph.conf: |
            [global]
            auth_cluster_required = cephx
            auth_service_required = cephx
            auth_client_required = cephx
        # keyring is a required key and its value should be empty
        keyring: |
    metadata:
        name: ceph-config
    ```

4. 获取`ceph`用户信息，找到`client.admin`用户，复制后边的Key，然后创建一个Secret对象：

    ```yaml
    # 文件：secret-csi-rbd.yaml
    # 名称：csi-rbd-secret
    apiVersion: v1
    kind: Secret
    metadata:
        name: csi-rbd-secret
        namespace: default
    stringData:
        userID: client.admin
        userKey: xxx
    ```

5. 最终结果如下：

    ```shell
    kubectl get secret
    kubectl get cm
    ```

    ![](./_image/2022-11-30/20221130144434.png)

### 2.3.K8S: RBAC

1. `ceph-csi`需要创建ServiceAccount和RBAC用于访问集群内部信息，先下载yaml文件：

    ```shell
    # csi-provisioner-rbac.yaml
    wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
    
    # csi-nodeplugin-rbac.yaml
    wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml
    # /root/deploy/ceph目录下
    ll
    ```

    ![](./_image/2022-11-30/20221130145027.png)

2. 将下载的两个yaml文件内容配置到K8S中：

    ```shell
    kubectl apply -f csi-provisioner-rbac.yaml
    kubectl apply -f csi-nodeplugin-rbac.yaml
    ```

    ![](./_image/2022-11-30/20221130145202.png)

### 2.4.K8S: provisioner

1. 下载两个yaml文件到本地：

    ```shell
    # csi-rbdplugin-provisioner.yaml
    wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
    
    # csi-rbdplugin.yaml
    wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml

    ll
    ```

    ![](./_image/2022-11-30/20221130151257.png)

2. 查看第一个yaml文件中的镜像，手动拉取镜像（推荐）

    ```shell
    # csi-rbdplugin-provisioner.yaml
    # Resizer
    docker pull registry.k8s.io/sig-storage/csi-resizer:v1.6.0
    # Attacher
    docker pull registry.k8s.io/sig-storage/csi-attacher:v4.0.0
    # Snapshotter
    docker pull registry.k8s.io/sig-storage/csi-snapshotter:v6.1.0
    # Provisioner
    docker pull registry.k8s.io/sig-storage/csi-provisioner:v3.3.0

    # csi-rbdplugin.yaml
    docker pull registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.0

    # 关键镜像（两个文件都需要）
    docker pull quay.io/cephcsi/cephcsi:canary
    ```

3. 将两个配置文件内容配置到K8S中：

    ```shell
    kubectl apply -f csi-rbdplugin.yaml
    kubectl apply -f csi-rbdplugin-provisioner.yaml
    ```

## 3. 问题

### 3.1. FailedScheduling

```shell
Warning  FailedScheduling  2m16s  default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 node(s) didn't match pod anti-affinity rules. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 node(s) didn't match pod anti-affinity rules.
```

![](./_image/2022-11-30/20221130215407.png)

&ensp;&ensp;&ensp;&ensp;这个问题的主要原因是K8S的Master节点默认是不参与调度的，且在Master节点上有一个**污点**NoSchedule（表示K8S不会将Pod调度到具有污点的Node上），若想让master节点参与调度，则需要先删除污点，允许k8s将Pod调度到该节点Node上，再添加master为nodes角色。

1. 先查看node以及污点

    ```shell
    kubectl get nodes
    kubectl describe node k8s-master |grep Taints
    ```

    ![](./_image/2022-11-30/20221130221150.png)

2. 删除污点

    ```shell
    kubectl taint nodes --all node-role.kubernetes.io/control-plane-
    ```

    ![](./_image/2022-11-30/20221130221255.png)

3. 再查看Pods运行情况：

    ```shell
    kubectl get pods --all-namespaces -o wide
    ```

    ![](./_image/2022-11-30/20221130221628.png)

[^1]: [kubernetes ceph-csi分析目录导航（系列）](https://blog.csdn.net/kyle18826138721/article/details/115531669), 作者：[良凯尔](https://blog.csdn.net/kyle18826138721?type=blog)
